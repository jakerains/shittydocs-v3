import { SYSTEM_PROMPT } from '../config/systemPrompt';
import { chat, getEnhancedResponse } from './llm';
import type { LLMError } from './llm/types';
import type { StreamCallbacks } from './llm/deepseekProvider';

interface ChatResponse {
  content: string;
  suggestions: string[];
  reasoningContent?: string;
}

export async function getChatResponse(
  userPrompt: string, 
  useDeepThought = false,
  streamCallbacks?: StreamCallbacks
): Promise<ChatResponse> {
  try {
    let response;
    
    if (useDeepThought) {
      response = await getEnhancedResponse(userPrompt, SYSTEM_PROMPT, streamCallbacks);
    } else {
      response = await chat(userPrompt, SYSTEM_PROMPT);
    }
    
    if (!response.content || response.content.trim().length === 0) {
      throw new Error("Got an empty response from the AI");
    }

    // Generate follow-up suggestions
    const suggestionsPrompt = `Based on the previous response about "${userPrompt}", 
    generate 4 follow-up questions in our signature style. Make them interesting and 
    related to what was just explained. Return ONLY the questions, one per line, 
    no numbers or bullets. Keep them short and punchy.`;
    
    const { content: suggestionsContent } = await chat(suggestionsPrompt, SYSTEM_PROMPT);
    const suggestions = suggestionsContent
      .split('\n')
      .filter(line => line.trim().length > 0)
      .slice(0, 4);

    // Ensure content is a string and add disclaimer
    const contentWithDisclaimer = `${response.content}\n\n---\n\n*This shitty content is generated by AI. Don't be a shithead and take it as fact. Be sure to double check everything.*`;
    return { 
      content: contentWithDisclaimer, 
      suggestions,
      reasoningContent: response.reasoningContent 
    };
  } catch (error) {
    const errorMessage = error instanceof Error ? error.message : 'Failed to get a response. Try that shit again later.';
    throw new Error(errorMessage);
  }
}